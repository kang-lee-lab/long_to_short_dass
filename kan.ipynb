{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from kan import KAN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "import torch\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (classification_report, balanced_accuracy_score, confusion_matrix, \n",
    "                roc_auc_score, accuracy_score, roc_curve, RocCurveDisplay)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(data, confidence=0.95):\n",
    "    # Calculate confidence interval\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2.0, n-1)\n",
    "    return m-h, m+h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_numbers = [1, 2, 3, 4, 5, 6, 7, 8]         # Numbers of questions from DASS to run through\n",
    "target = \"anxiety_status\"\n",
    "models_to_train = 10        # Number of models for each number of questions from DASS\n",
    "models_per_question = 50    # Number of ensembles per model\n",
    "test_split = 0.1    \n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "ACCS = []\n",
    "AUCS = []\n",
    "PRES = []\n",
    "RECS = []\n",
    "F1S = []\n",
    "AUC_STDEV = []\n",
    "F1_STDEV = []\n",
    "AUC_95CI_U = []\n",
    "AUC_95CI_D = []\n",
    "F1_95CI_U = []\n",
    "F1_95CI_D = []\n",
    "\n",
    "\n",
    "data_folder = \"./data\"\n",
    "models_folder = \"./models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df = pd.read_csv(os.path.join(data_folder, \"features.csv\"))\n",
    "labels_df = pd.read_csv(os.path.join(data_folder, \"labels.csv\"))\n",
    "\n",
    "questions = [20, 9, 30, 11, 19, 2, 36, 28, 4, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_acc():\n",
    "                return torch.mean((torch.round(model(dataset['train_input'])[:,0]) == dataset['train_label'][:,0]).float())\n",
    "\n",
    "def test_acc():\n",
    "    return torch.mean((torch.round(model(dataset['test_input'])[:,0]) == dataset['test_label'][:,0]).float())\n",
    "\n",
    "def acc(formula, X, y):\n",
    "                batch = X.shape[0]\n",
    "                correct = 0\n",
    "                for i in range(batch):\n",
    "                    correct += np.round(np.array(formula.subs('x_1', X[i,0]).subs('x_2', X[i,1])).astype(np.float64)) == y[i,0]\n",
    "                return correct/batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For different numbers of questions from DASS-42\n",
    "for num_questions in question_numbers:  # For question numbers in [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    models = {}\n",
    "\n",
    "    accs = []\n",
    "    aucs = []\n",
    "    pres = []\n",
    "    recs = []\n",
    "    f1s = []\n",
    "    auc_stdev = []\n",
    "    f1_stdev = []\n",
    "    auc_95ci_u = []\n",
    "    auc_95ci_d = []\n",
    "    f1_95ci_u = []\n",
    "    f1_95ci_d = []\n",
    "    lst_comb =[]    \n",
    "\n",
    "    model_num = 0\n",
    "    for a in range(models_to_train):  # For a in range(10)\n",
    "        model = {}\n",
    "\n",
    "        print(\"Training model\", a)\n",
    "        cols = [\"gender_m\", \"gender_f\", \"region_other\", \n",
    "                    \"region_east\", \"region_west\", \"age_norm\"] # With demographic features\n",
    "\n",
    "        if num_questions == 1:  # Only has 1 question\n",
    "            if a >= len(questions):\n",
    "                break\n",
    "            question_nums = [questions[a]]  # a=0, question[0]=20, question_nums contains a list of question numbers\n",
    "            \n",
    "        else:  # More than 1 question\n",
    "            question_nums = random.sample(questions, num_questions)  # Randomly sample num_questions from all questions\n",
    "            question_nums.sort() # Sort the questions\n",
    "            #resample if already in list\n",
    "            while question_nums in lst_comb:\n",
    "                question_nums = random.sample(questions, num_questions)\n",
    "            lst_comb.append(question_nums)\n",
    "        # Finish sampling questions\n",
    "\n",
    "        for q in question_nums:  # q is one of the selected questions\n",
    "            for j in range(4):\n",
    "                cols.append(\"Q{0}A_{1}\".format(q, j))  # Generate the question numbers\n",
    "        features = feats_df[cols]  # Get the features for the selected questions\n",
    "\n",
    "        labels = labels_df[[target]].copy()\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        shufId = np.random.permutation(int(len(labels)))\n",
    "        index = int(test_split * len(labels.index))  # Index of data to be used for testing\n",
    "\n",
    "        df_prist = features.iloc[shufId[0:index]]   # Data for testing\n",
    "        df_trainvalid = features.iloc[shufId[index:-1]] # Data for training and validation\n",
    "\n",
    "        gt_prist = labels.iloc[shufId[0:index]]     # Labels for testing\n",
    "        gt_trainvalid = labels.iloc[shufId[index:-1]]       # Labels for training and validation\n",
    "\n",
    "        df_prist.to_csv(os.path.join(data_folder, \"prist_features.csv\"), index=False)\n",
    "        gt_prist.to_csv(os.path.join(data_folder, \"prist_labels.csv\"), index=False)\n",
    "\n",
    "        accs1 = []\n",
    "        aucs1 = []\n",
    "        pres1 = []\n",
    "        recs1 = []\n",
    "        f1s1 = []\n",
    "        ensemble_models = []\n",
    "        \n",
    "        train_rmse = []\n",
    "        test_rmse = []\n",
    "        \n",
    "\n",
    "        for b in range(models_per_question):  # b from 0 to 49\n",
    "            if b % 10 == 0:\n",
    "                print(\"Training iteration\", b)\n",
    "\n",
    "            np.random.seed(b)\n",
    "            shufId = np.random.permutation(int(len(gt_trainvalid)))\n",
    "            index = int((1/9) * len(gt_trainvalid.index))\n",
    "\n",
    "            df_valid = df_trainvalid.iloc[shufId[0:index]]\n",
    "            df_train = df_trainvalid.iloc[shufId[index:-1]]\n",
    "\n",
    "            gt_valid = gt_trainvalid.iloc[shufId[0:index]]\n",
    "            gt_train = gt_trainvalid.iloc[shufId[index:-1]]\n",
    "\n",
    "            df_valid = df_valid.reset_index(drop=True)\n",
    "            df_train = df_train.reset_index(drop=True)\n",
    "\n",
    "            gt_valid = gt_valid.reset_index(drop=True)\n",
    "            gt_train = gt_train.reset_index(drop=True)\n",
    "\n",
    "            dataset = {}\n",
    "            # Convert data to PyTorch tensors\n",
    "            dataset['train_input'] = torch.from_numpy(df_train.values).float()\n",
    "            dataset['train_label'] = torch.from_numpy(gt_train.values[:, None]).float()\n",
    "            dataset['test_input'] = torch.from_numpy(df_prist.values).float()\n",
    "            dataset['test_label'] = torch.from_numpy(gt_prist.values[:, None]).float()\n",
    "            \n",
    "            \n",
    "            X = dataset['train_input']\n",
    "            y = dataset['train_label']\n",
    "            \n",
    "            # Determine the number of features (input dimensions) and output dimensions\n",
    "            input_dim = df_train.shape[1]  # Number of features\n",
    "            output_dim = 1  # Assuming binary classification or regression for simplicity\n",
    "\n",
    "            # Define the width of the network layers\n",
    "            # Example: input layer, two hidden layers with 5 neurons each, and output layer\n",
    "            width = [input_dim, 5, output_dim] \n",
    "            \n",
    "            # Initialize and train the KAN model\n",
    "            model = KAN(\n",
    "                        width=width,\n",
    "                        grid=3, \n",
    "                        k=3   \n",
    "                    )\n",
    "            def train_acc():\n",
    "                return torch.mean((torch.round(model(dataset['train_input'])[:,0]) == dataset['train_label'][:,0]).float())\n",
    "\n",
    "            def test_acc():\n",
    "                return torch.mean((torch.round(model(dataset['test_input'])[:,0]) == dataset['test_label'][:,0]).float())\n",
    "\n",
    "            results = model.train(dataset, opt=\"LBFGS\", steps=20, metrics=(train_acc, test_acc))\n",
    "            lib = ['x','x^2','x^3','x^4','exp','log','sqrt','tanh','sin','tan','abs']\n",
    "            model.auto_symbolic(lib=lib)\n",
    "            formula = model.symbolic_formula()[0][0]\n",
    "            print(formula)\n",
    "            train_rmse.append(results['train_loss'][-1].item())\n",
    "            test_rmse.append(results['test_loss'][-1].item())\n",
    "            \n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
