{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670d6b22-468a-4d4c-9f5c-32353c69e4e7",
   "metadata": {},
   "source": [
    "# Feature importance based on Minimum Redundancy Maximum Relevance (MRMR) and Extra Tree (ET) criteria\n",
    "\n",
    "This file will identify the most important 20 features (i.e. questions) in the DASS-42 (Depression Anxiety and Stress Scale-42). \n",
    "\n",
    "Feature selection is done to select the questions that were best to predict whether participants’ final anxiety scores were at the low or high class.\n",
    "\n",
    "# 1. Feature selection based on Minimum Redundancy Maximum Relevance (MRMR) criteria\n",
    "Firstly, import the necessary libraries and define constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "265868dd-b221-4ed2-adba-5741f976db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "# import pymrmr # Not available anymore. Use the customized function instead.\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# from skfda.preprocessing.dim_reduction import variable_selection\n",
    "# import skfda\n",
    "# import numpy as np\n",
    "# import dcor\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mutual_info_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "981d8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data folder\n",
    "data_folder = \"./data\"\n",
    "show_top = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f19a99e",
   "metadata": {},
   "source": [
    "Using the one-hot encoded features dataset, print the minimum number (20) of useful features using mRMR. \n",
    "\n",
    "To see all of the features and their corresponding mRMR scores: Run Jupyter Notebook through a CLI (Anaconda Prompt for Windows or your built-in command line applications for MacOS or Linux)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0db355f-1145-404b-8f8b-ece98934eebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 32.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q41A_0', 'Q36A_1', 'Q28A_0', 'Q7A_0', 'Q19A_0', 'Q4A_0', 'Q40A_3', 'Q20A_0', 'Q25A_0', 'Q9A_3', 'Q36A_0', 'Q15A_0', 'Q30A_0', 'Q12A_0', 'Q36A_3', 'Q23A_0', 'Q11A_3', 'Q22A_0', 'Q2A_0', 'Q33A_0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoded dataset\n",
    "# Load data\n",
    "########################## Customized MRMR Approach ##########################\n",
    "\n",
    "# def MRMR(X, y, k):\n",
    "#     n_samples, n_features = X.shape\n",
    "#     S = [i for i in range(n_features)] \n",
    "#     F = []\n",
    "\n",
    "#     MIf = np.zeros(n_features)\n",
    "#     for i in range(n_features):\n",
    "#         f = X[:, i]\n",
    "#         MIf[i] = mutual_info_score(y,f)\n",
    "    \n",
    "#     f_star = np.argmax(MIf)\n",
    "#     F.append(f_star)\n",
    "#     S.remove(f_star)\n",
    "    \n",
    "#     while len(F) < k:\n",
    "#         mrmr = np.zeros(len(S))\n",
    "#         for i, f in enumerate(S):\n",
    "#             f_s = F + [f]\n",
    "#             mrmr[i] = MIf[f] - np.mean([MIf[j] for j in f_s])\n",
    "        \n",
    "#         f_star = S[np.argmax(mrmr)]\n",
    "#         F.append(f_star)\n",
    "#         S.remove(f_star)\n",
    "#     return F\n",
    "\n",
    "# train_feats = pd.read_csv(os.path.join(data_folder, \"features.csv\"))\n",
    "# train_feats = train_feats.drop([\"age_norm\", \"gender_m\", \"gender_f\", \"region_other\", \"region_east\", \"region_west\"], axis=1)  # Comment this line to include demographics\n",
    "# labels = pd.read_csv(os.path.join(data_folder, \"labels.csv\"))\n",
    "\n",
    "# print(MRMR(train_feats.to_numpy(), labels.to_numpy().ravel(), show_top))\n",
    "\n",
    "########################## From Library ##########################\n",
    "import mrmr\n",
    "from mrmr import mrmr_classif\n",
    "\n",
    "\n",
    "train_feats = pd.read_csv(os.path.join(data_folder, \"features.csv\"))\n",
    "train_feats = train_feats.drop([\"age_norm\", \"gender_m\", \"gender_f\", \"region_other\", \"region_east\", \"region_west\"], axis=1)  # Comment this line to include demographics\n",
    "labels = pd.read_csv(os.path.join(data_folder, \"labels.csv\"))\n",
    "\n",
    "labels = labels[\"anxiety_status\"]\n",
    "\n",
    "selected_features = mrmr_classif(X=train_feats, y=labels, K=show_top)\n",
    "print(selected_features)\n",
    "\n",
    "########################## Depreciated Code ##########################\n",
    "# # Check for any completely NaN columns\n",
    "# print(train_feats.isna().all())\n",
    "\n",
    "# # Check for any columns with zero variance\n",
    "# print(train_feats.nunique(dropna=False))\n",
    "# # Removing columns that are entirely NaN or have zero variance\n",
    "# train_feats = train_feats.dropna(axis=1, how='all')  # Drops columns where all values are NaN\n",
    "# train_feats = train_feats.loc[:, train_feats.nunique(dropna=False) > 1]  # Drop columns with zero variance\n",
    "# # Fill NaN values, for example, with the mean of each column\n",
    "# train_feats.fillna(train_feats.mean(), inplace=True)\n",
    "\n",
    "# train_feats = train_feats.to_numpy()\n",
    "# labels = labels.to_numpy()\n",
    "\n",
    "# # Ensure `labels` is properly shaped for scikit-learn\n",
    "# if labels.ndim > 1:\n",
    "#     labels = labels.ravel()\n",
    "\n",
    "# feat_selector = mifs.MutualInformationFeatureSelector(method='MRMR', n_features=show_top)\n",
    "\n",
    "# try:\n",
    "#     feat_selector.fit(train_feats, labels)\n",
    "#     print(\"Features selected:\", feat_selector.support_)\n",
    "# except Exception as e:\n",
    "#     print(\"An error occurred during feature selection:\", e)\n",
    "\n",
    "\n",
    "# mrmr = variable_selection.MinimumRedundancyMaximumRelevance(\n",
    "#     n_features_to_select=3,\n",
    "#     method=\"MID\",\n",
    "# )\n",
    "# _ = mrmr.fit(train_feats, labels)\n",
    "# point_mask = mrmr.get_support()\n",
    "# points = train_feats.grid_points[0][point_mask]\n",
    "\n",
    "# print(_)\n",
    "# print(\"Processed dataset:\")\n",
    "# print(pymrmr.mRMR(train_feats, 'MIQ', show_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e0cc4d-8c09-47a8-840d-bef4628cd017",
   "metadata": {},
   "source": [
    "Using the not one-hot encoded dataset, print the minimum number (20) of useful features using mRMR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b3e84238-1849-4e19-abdf-3e56c9a084fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 72.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q28A', 'Q25A', 'Q36A', 'Q40A', 'Q41A', 'Q20A', 'Q7A', 'Q9A', 'Q12A', 'Q4A', 'Q30A', 'Q33A', 'Q19A', 'Q15A', 'Q1A', 'Q2A', 'Q29A', 'Q8A', 'Q11A', 'Q13A']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Not one-hot encoded dataset\n",
    "train_feats = pd.read_csv(os.path.join(data_folder, \"data_filtered_1000.csv\")) # Using sample dataset. Change the file name accordingly if using another dataset.\n",
    "train_feats = train_feats.drop([\"country\",\"gender\",\"age\",\"agegroup\",\"continent\",\"region\",\"anxiety_score\", \"anxiety_status\"], axis=1)\n",
    "\n",
    "labels = pd.read_csv(os.path.join(data_folder, \"data_filtered_1000.csv\"))\n",
    "labels = labels[\"anxiety_status\"]\n",
    "\n",
    "# print(MRMR(train_feats.to_numpy(), labels.to_numpy().ravel(), show_top)) # The customized function approach\n",
    "selected_features = mrmr_classif(X=train_feats, y=labels, K=show_top) # The mrmr library approach\n",
    "print(selected_features)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# region = label_encoder.fit_transform(train_feats[\"region\"])\n",
    "# region = pd.DataFrame(region)\n",
    "# region.columns = [\"region1\"]\n",
    "# train_feats = pd.concat([train_feats, region], axis=1)\n",
    "\n",
    "# train_feats = train_feats.drop([\"anxiety_score\", \"anxiety_status\", \"country\", \"agegroup\", \"continent\", \"region\"], axis=1)\n",
    "# train_feats = train_feats.drop([\"gender\", \"age\", \"region1\"], axis=1)  # Comment this line to include demographics\n",
    "\n",
    "# print(\"\\nUnprocessed dataset:\")\n",
    "# print(pymrmr.mRMR(train_feats, 'MIQ', show_top))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6da24-7c12-4c6e-89af-3ee6bd6ca9f6",
   "metadata": {},
   "source": [
    "# 2. Feature selection based on the Gini importance of features in an Extra Tree (ET) classifier\n",
    "\n",
    "This section will use an Extra-Tree Classifier to identify the most important 20 features (i.e., questions) by extracting the Gini importance of each feature from an Extra-Tree Classifier.\n",
    "\n",
    "Firstly, import the necessary libraries and define the variables used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "228e8ea5-f906-47ca-a7c8-d5afed96f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fae7da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data folder\n",
    "data_folder = \"./data\"\n",
    "show_top = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6ebed1-bfe3-44d8-bb35-3e0c1e6613fa",
   "metadata": {},
   "source": [
    "Using the one-hot encoded dataset, fit an Extra-Tree Classifier model and plot a graph of the top 20 features by Gini importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f90972b-08fd-4f08-afd4-f7fa89eee2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoded dataset\n",
    "features = pd.read_csv(os.path.join(data_folder, \"features.csv\"))\n",
    "labels = pd.read_csv(os.path.join(data_folder, \"labels.csv\"))\n",
    "features = features.drop([\"gender_m\", \"gender_f\", \"region_other\", \"region_east\", \"region_west\", \"age_norm\"], axis=1)  # Comment this line to include demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e351b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model for questions plus demographics\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(features, labels)\n",
    "# print(model.feature_importances_) # Use inbuilt class feature_importances of tree based classifiers (Gini importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=features.columns)\n",
    "feat_importances.nlargest(show_top).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31970f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List top 20 features\n",
    "print(feat_importances.nlargest(show_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06567d-52d4-480f-bbab-be54c93a7513",
   "metadata": {},
   "source": [
    "Using the not one-hot encoded dataset, fit an Extra-Tree Classifier model and plot a graph of the top 20 features by Gini importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34cabee-c2d6-4508-abb1-9ef3c9e0a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not one-hot encoded dataset\n",
    "df = pd.read_csv(os.path.join(data_folder, \"data_filtered_1000.csv\")) # Using sample dataset.\n",
    "features = df\n",
    "labels = df[\"anxiety_status\"]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "region = label_encoder.fit_transform(features[\"region\"])\n",
    "region = pd.DataFrame(region)\n",
    "region.columns = [\"region1\"]\n",
    "features = pd.concat([features, region], axis=1)\n",
    "\n",
    "features = features.drop([\"anxiety_score\", \"anxiety_status\", \"country\", \"agegroup\", \"continent\", \"region\"], axis=1)\n",
    "features = features.drop([\"gender\", \"age\", \"region1\"], axis=1)  # Comment this line to include demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852221c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model for questions plus demographics\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(features, labels)\n",
    "# print(model.feature_importances_) # Use inbuilt class feature_importances of tree based classifiers (Gini importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11899498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=features.columns)\n",
    "feat_importances.nlargest(show_top).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908716f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List top 20 features\n",
    "print(feat_importances.nlargest(show_top))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
